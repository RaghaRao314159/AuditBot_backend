{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain libraries\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# huggingface libraries\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Add the parent directory (Auditbot_backend) to the system path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.dirname(f\"{os.getcwd()}/inmemory_retriever.ipynb\")\n",
    "            , '..'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import OPENAI_API_KEY, s_p_pairs_path\n",
    "\n",
    "# custom helper functions\n",
    "from utils.json_parser import json_file_to_dict\n",
    "\n",
    "# Other useful libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 8210\n"
     ]
    }
   ],
   "source": [
    "# Load helper data_structures (page number : headings/sections)\n",
    "it_path = \"../data/parsed_documents/inverted_tree.json\"\n",
    "inverted_tree = json_file_to_dict(it_path)\n",
    "\n",
    "# get chunks from tree's keys\n",
    "chunks = list(inverted_tree.keys())\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "\n",
    "query = \"What are the findings pertaining to grant?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_docs_to_textlist(langchain_docs):\n",
    "    \"\"\"\n",
    "    @param langchain_docs: list, collection of langchain document class objects\n",
    "    @return textlist: list, text (str) obtained from langchain object\n",
    "    \"\"\"\n",
    "    textlist = []\n",
    "    for doc in langchain_docs:\n",
    "        textlist.append(doc.page_content)\n",
    "    \n",
    "    return textlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(chunks, query, top_k):\n",
    "    chunk_list_1 = chunks\n",
    "    chunk_list_2 = chunks.copy()\n",
    "    \n",
    "    # initialize the bm25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_texts(\n",
    "        chunk_list_1\n",
    "    )\n",
    "    bm25_retriever.k = top_k\n",
    "\n",
    "    # initialize the faiss retriever\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key = OPENAI_API_KEY)\n",
    "    faiss_vectorstore = FAISS.from_texts(\n",
    "        chunk_list_2, embedding\n",
    "    )\n",
    "    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    "    )\n",
    "\n",
    "    good_langchain_docs = ensemble_retriever.invoke(query)\n",
    "    good_chunks = langchain_docs_to_textlist(good_langchain_docs)\n",
    "\n",
    "    return good_chunks, good_langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_chunks, good_langchain_docs = ranking(chunks, query, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Details of the lapses pertaining to the enforcement of SDL collections are in the \n",
      " \n",
      "following paragraphs\n",
      "page_content='Details of the lapses pertaining to the enforcement of SDL collections are in the \\n \\nfollowing paragraphs'\n",
      "----------------------------------------------------------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether processes were in place to ensure that grant programmes \n",
      "were authorised and reviewed for relevance\n",
      "b\n",
      "page_content='Stage 1: Grant Design and Setup\\n– whether processes were in place to ensure that grant programmes \\nwere authorised and reviewed for relevance\\nb'\n",
      "----------------------------------------------------------------\n",
      "Audit findings are conveyed by AGO to the ministries and statutory boards audited \n",
      "by way of “management letters”\n",
      "page_content='Audit findings are conveyed by AGO to the ministries and statutory boards audited \\nby way of “management letters”'\n",
      "----------------------------------------------------------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "page_content='Stage 1: Grant Design and Setup\\n– whether there were processes and controls in place to ensure that \\ngrant programmes were authorised and administered in accordance \\nwith the objective(s) of the grant'\n",
      "----------------------------------------------------------------\n",
      "Audit findings are conveyed to the Government ministries, statutory boards and other \n",
      "entities audited by way of “management letters”\n",
      "page_content='Audit findings are conveyed to the Government ministries, statutory boards and other \\nentities audited by way of “management letters”'\n",
      "----------------------------------------------------------------\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grant cases were properly evaluated and approved\n",
      "page_content='Stage 2: Grant Evaluation and Approval\\n–\\t\\nWhether there were processes and controls in place to ensure \\nthat grant cases were properly evaluated and approved'\n",
      "----------------------------------------------------------------\n",
      "(c)\t\n",
      "Briefing the auditees on IA findings and obtaining feedback prior to \n",
      "issuing the final audit report\n",
      "page_content='(c)\\t\\nBriefing the auditees on IA findings and obtaining feedback prior to \\nissuing the final audit report'\n",
      "----------------------------------------------------------------\n",
      "Application, evaluation and award of grants \n",
      "– whether the processes to invite, receive, evaluate and approve \n",
      "proposals and contract with grant recipients2 were properly administered\n",
      "b\n",
      "page_content='Application, evaluation and award of grants \\n– whether the processes to invite, receive, evaluate and approve \\nproposals and contract with grant recipients2 were properly administered\\nb'\n",
      "----------------------------------------------------------------\n",
      "PUB explained that the purchases were not deliberately split to avoid \n",
      "complying with requirements pertaining to higher value purchases\n",
      "page_content='PUB explained that the purchases were not deliberately split to avoid \\ncomplying with requirements pertaining to higher value purchases'\n",
      "----------------------------------------------------------------\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant applications were properly evaluated and approved\n",
      "page_content='Stage 2: Grant Evaluation and Approval\\n– whether there were processes and controls in place to ensure that \\ngrant applications were properly evaluated and approved'\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(len(good_chunks))\n",
    "for chunk, langchain_doc in zip(good_chunks, good_langchain_docs):\n",
    "    print(chunk)\n",
    "    print(langchain_doc)\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking(model_name, good_chunks, query, top_n = -1):\n",
    "    model = CrossEncoder(model_name, max_length=512)\n",
    "    queries = [query for _ in range(len(good_chunks))] \n",
    "    scores = model.predict(list(zip(queries, good_chunks)))\n",
    "    print(\"scores:\", scores)\n",
    "    best_idxs = np.argsort(scores)[::-1]\n",
    "    print(\"best idxs:\", best_idxs)\n",
    "\n",
    "    best_chunks = []\n",
    "    for k, idx in enumerate(best_idxs):\n",
    "        if k >= top_n:\n",
    "            break\n",
    "        best_chunks.append(good_chunks[idx])\n",
    "    \n",
    "    return best_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "scores: [0.10725074 0.2886162  0.1900847  0.27472395 0.18794931 0.34453192\n",
      " 0.16118707 0.3218784  0.074872   0.34653497]\n",
      "best idxs: [9 5 7 1 3 2 4 6 0 8]\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant applications were properly evaluated and approved\n",
      "----------------------------------------------------------------\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grant cases were properly evaluated and approved\n",
      "----------------------------------------------------------------\n",
      "Application, evaluation and award of grants \n",
      "– whether the processes to invite, receive, evaluate and approve \n",
      "proposals and contract with grant recipients2 were properly administered\n",
      "b\n",
      "----------------------------------------------------------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether processes were in place to ensure that grant programmes \n",
      "were authorised and reviewed for relevance\n",
      "b\n",
      "----------------------------------------------------------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_chunks = reranking(\"cross-encoder/ms-marco-MiniLM-L-12-v2\", good_chunks, query, 5)\n",
    "\n",
    "for chunk in best_chunks:\n",
    "    print(chunk)\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_p_pair = json_file_to_dict(s_p_pairs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: Grant Evaluation and Approval\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant applications were properly evaluated and approved; and\n",
      "– whether agreements with grant recipients were properly entered into\n",
      "-------------\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grant cases were properly evaluated and approved; and\n",
      "–\t\n",
      "Whether proper terms and conditions were stipulated for \n",
      "compliance\n",
      "-------------\n",
      "Application, evaluation and award of grants \n",
      "– whether the processes to invite, receive, evaluate and approve \n",
      "proposals and contract with grant recipients2 were properly administered\n",
      "b\n",
      "-------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether processes were in place to ensure that grant programmes \n",
      "were authorised and reviewed for relevance\n",
      "b\n",
      "-------------\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in best_chunks:\n",
    "    paragraph = s_p_pair[chunk]\n",
    "    print(paragraph)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in ensemble retriever, text chunks are taken as dictionary keys and the \n",
    "# metadata are the values. \n",
    "\n",
    "# If the same text chunk is used with different meta data, the first one loaded\n",
    "# into dictionary is used as values. \n",
    "\n",
    "# This just means the metadata tagged to the chunks are useless to us as we use\n",
    "# the same document for both retrievers. It is only useful if both retrievers \n",
    "# use different documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 1}),\n",
       " Document(page_content='I like Pi', metadata={'source': 1}),\n",
       " Document(page_content='Apples and oranges are fruits', metadata={'source': 2})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is overlap \n",
    "\n",
    "doc_list_1 = [\n",
    "    \"I like apples\",\n",
    "    \"I like oranges\",\n",
    "    \"Apples and oranges are fruits\",\n",
    "    \"I like chimps\",\n",
    "    \"I like sydney\",\n",
    "    \"I like kangaroos\",\n",
    "    \"I like Pi\"\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key = OPENAI_API_KEY)\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_1, embedding, metadatas=[{\"source\": 2}] * len(doc_list_1)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 1}),\n",
       " Document(page_content='I like kangaroos', metadata={'source': 1}),\n",
       " Document(page_content='I like Pi', metadata={'source': 1}),\n",
       " Document(page_content='Apples and oranges are fruits', metadata={'source': 1}),\n",
       " Document(page_content='I like oranges', metadata={'source': 1}),\n",
       " Document(page_content='I like chimps', metadata={'source': 1}),\n",
       " Document(page_content='I like sydney', metadata={'source': 1})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include all\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 7\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key = OPENAI_API_KEY)\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_1, embedding, metadatas=[{\"source\": 2}] * len(doc_list_1)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 2}),\n",
       " Document(page_content='I like kangaroos', metadata={'source': 2}),\n",
       " Document(page_content='Apples and oranges are fruits', metadata={'source': 2}),\n",
       " Document(page_content='I like Pi', metadata={'source': 2}),\n",
       " Document(page_content='I like oranges', metadata={'source': 2}),\n",
       " Document(page_content='I like chimps', metadata={'source': 2}),\n",
       " Document(page_content='I like sydney', metadata={'source': 2})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch order models are added to ensemble\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 7\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key = OPENAI_API_KEY)\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_1, embedding, metadatas=[{\"source\": 2}] * len(doc_list_1)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm25_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"apples\")\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
