{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (Auditbot_backend) to the system path\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.dirname(f\"{os.getcwd()}/RAG.ipynb\"),\n",
    "            '..'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG to Build a Custom ChatBot\n",
    "## 3. Retrieval-Augmented Generation\n",
    "\n",
    "> **Notice:**  \n",
    "> Before starting this tutorial series, read up on the RAG pipeline.\n",
    "\n",
    "This tutorial series assumes prerequisite understanding of RAG and therefore goes through the implementation of an advanced and customized RAG pipeline, explaining the micro-decisions made along the way.\n",
    "\n",
    "> **Data Corpus:** \n",
    "> This tutorial uses [AGO yearly audit reports](https://www.ago.gov.sg/publications/annual-reports/) as an example. However, this repo's code is applicable to most pdf documents. The code examples for other documents (such as national day rally) will be referenced later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load vector store\n",
    "\n",
    "We need to load up the embedding vectors in our chroma db to perform dense retrieval. Here is the syntax for that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromadb library\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# custom helper functions\n",
    "from utils.db_utils import chroma_get_or_create_collection \n",
    "\n",
    "# constants\n",
    "from utils.initialisations import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dense = chromadb.PersistentClient(path=\"../data/db\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "\n",
    "# make sure to set reset = False as we are only loading already saved data\n",
    "collection = chroma_get_or_create_collection(client_dense, \n",
    "                                             name = \"audit\", \n",
    "                                             embedding_function = openai_ef, \n",
    "                                             reset = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load index\n",
    "\n",
    "We need to load up the string index in the elastic container to perform sparce retrieval. Here is the syntax for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic search library\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import LOCAL_HOST_URL, HTTP_AUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_sparce = Elasticsearch(\n",
    "    LOCAL_HOST_URL,\n",
    "    basic_auth=HTTP_AUTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform dense retrieval\n",
    "\n",
    "ChromaDB comes with the functionality of performing dense retrieval using our choice of bi-encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.db_utils import chromadb_embedding_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be run multiple times when change in query\n",
    "query = \"What are the findings pertaining to grant?\"\n",
    "top_k = 30\n",
    "\n",
    "# dense embeding search \n",
    "embedding_results = chromadb_embedding_search(collection, query, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to performing dense retrieval in chromadb is using [Facebook AI Similarity Search](https://ai.meta.com/tools/faiss/). Their efficient implementation of cosine similarity serch has allowed for very quick dense retrieval. \n",
    "\n",
    "Check out [\"../notebooks/inmemory_retriever.ipynb\"](../notebooks/inmemory_retriever.ipynb) to try FAISS using the AGO documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform sparce retrieval\n",
    "\n",
    "I chose a bag-of-words retrieval function (Okapi BM25) to perform sparce retrieval due to its success in traditional NLP projects and a plathora of documentation. Other good alternatives include TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "I used of ElasticSearch's implementation of [\"BM25\"\"](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables) to perform sparce retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper functions\n",
    "from utils.db_utils import bm25_elasticsearch\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bm_25 using elasticsearch\n",
    "bm25_results = bm25_elasticsearch(client_sparce, index_name, HTTP_AUTH, query, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to performing dense retrieval in ElasticSearch is using [LangChain's BM25 function](https://python.langchain.com/v0.2/docs/integrations/retrievers/bm25/). However, they do not index the documents before performing BM25 search so it will not be as efficient as ElasticSearch. For langchain's BM25 search, data has to be loaded in from in-code memory (no use of data storage unlike elastic and chromadb).\n",
    "\n",
    "Check out [\"../notebooks/inmemory_retriever.ipynb\"](../notebooks/inmemory_retriever.ipynb) to try BM25Retriever from LangChain using the AGO documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Combine results of both retrievals (Ranking)\n",
    "Both retrievals produce a rank of what they think are the chunks closely related to the query. Since they probably conflict, we need to fuse both ranks to produce a combined rank. I went with [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) as it allows us to tune the combined rank to produce the best output we want. \n",
    "\n",
    "A custom RRF function was written for this step as I added weights to each rank. The definition of this function ```reciprocal_rank_fusion``` can be found in [\"../utils/db_utils.py\"](../utils/db_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.db_utils import reciprocal_rank_fusion\n",
    "from utils.custom_print import pretty_print_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED RANKING---------------------------------------------------------\n",
      "\n",
      "idx: 0\n",
      "\n",
      "Details of the lapses pertaining to the enforcement of SDL collections are in the \n",
      " \n",
      "following paragraphs\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "idx: 1\n",
      "\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "idx: 2\n",
      "\n",
      "Audit findings are conveyed by AGO to the ministries and statutory boards audited \n",
      "by way of “management letters”\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weights for each retrieval for reciprocal rank fusion\n",
    "weights = [0.5, 0.5]\n",
    "\n",
    "# reciprocal ranking fusion constant\n",
    "k = 60\n",
    "\n",
    "# RRF\n",
    "good_chunks  = reciprocal_rank_fusion(bm25_results, \n",
    "                                 embedding_results, \n",
    "                                 weights, \n",
    "                                 k)\n",
    "\n",
    "print(\"COMBINED RANKING---------------------------------------------------------\\n\")\n",
    "pretty_print_list(good_chunks[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to performing RRF is using [LangChain's Ensemble Retriever](https://python.langchain.com/docs/how_to/ensemble_retriever/). \n",
    "\n",
    "This function appears to behave as follows:\n",
    "1. combine both sparce and dense retrievers into an ensemble retriever\n",
    "2. perform a single retreival using the newly created ensemble retreival \n",
    "\n",
    "However, that is not true. Under the hood, it still performs both dense and sparce retrieval seperately and combines both results using RRF. Therefore, it is similar to our custom setup. Ensemble retrieval can however exhibit unexpected behaviours. \n",
    "\n",
    "I have carried out some experiments with this Ensemble Retriever at the end of [\"../notebooks/inmemory_retriever.ipynb\"](../notebooks/inmemory_retriever.ipynb) and explained how it works. This notebook also uses Ensemble Retriever on AGO reports as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: ReRanking\n",
    "\n",
    "I have picked 2 models. \n",
    "\n",
    "1. A familiar and light model, [RoBERTa](https://arxiv.org/abs/1907.11692). Various versions of [opensource RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) exist so feel free to explore before choosing one.\n",
    "\n",
    "2. For higher accuracy (but increased difficulty of setup), use the Microsoft Machine Reading Comprehension Leaderboard ([MS MACRO](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-2-v2)) or the HuggingFace Massive Text Embedding Benchmark Leaderboard ([MTEB](https://huggingface.co/spaces/mteb/leaderboard)) to pick the best cross-encoders for reranking. The 2nd model chosen is ms-marco-MiniLM-L-12-v2, the best performing model on MS-MACRO leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.retriever import reranking\n",
    "from utils.custom_print import pretty_print_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "RERANKING-----------------------------------------------------------\n",
      "\n",
      "RANK: 1\n",
      "\n",
      "Pertaining to the lack of checks on declarations by grant recipients, EDB \n",
      "explained that there were specific controls in place to ensure that grant recipients take \n",
      "ownership for accurate and credible reporting\n",
      "\n",
      "SCORE: 3.238358\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 2\n",
      "\n",
      "The audit examined whether there was a proper framework for grant \n",
      "management and whether due process was followed for the above stages\n",
      "\n",
      "SCORE: -0.9363227\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 3\n",
      "\n",
      "The audit examined whether there was a proper framework for grant \n",
      "management and whether due process was followed for the above stages by the two \n",
      "agencies\n",
      "\n",
      "SCORE: -1.0825868\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 4\n",
      "\n",
      "Stage 4: Grant Monitoring and Review\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grants were managed in accordance with relevant terms \n",
      "and conditions, and that the deliverables were achieved\n",
      "\n",
      "SCORE: -1.2081927\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 5\n",
      "\n",
      "Stage 4: Grant Monitoring and Review\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grants were managed in accordance with relevant terms and conditions, \n",
      "and that the deliverables were achieved\n",
      "\n",
      "SCORE: -1.2081927\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 6\n",
      "\n",
      "The audit covered five stages of grant management, namely, \n",
      "(i) grant design and setup, (ii) grant evaluation and approval, (iii) disbursement of grants, \n",
      "(iv) monitoring and review of grants, and (v) cessation of grants\n",
      "\n",
      "SCORE: -1.2928011\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 7\n",
      "\n",
      "Audit Observations\n",
      "In this year’s audits, AGO uncovered a number of instances that indicated laxity in \n",
      "the administration of grants\n",
      "\n",
      "SCORE: -1.5757418\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 8\n",
      "\n",
      "To apply for the grant, applicants are required to identify the \n",
      "equipment or digital solution that they intended to procure\n",
      "\n",
      "SCORE: -2.4338026\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 9\n",
      "\n",
      "Application, evaluation and award of grants \n",
      "– whether the processes to invite, receive, evaluate and approve \n",
      "proposals and contract with grant recipients2 were properly administered\n",
      "b\n",
      "\n",
      "SCORE: -2.4415512\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 10\n",
      "\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grant cases were properly evaluated and approved\n",
      "\n",
      "SCORE: -2.568812\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 11\n",
      "\n",
      "Disbursement of grants \n",
      "– whether the processes were in place to ensure that grants were \n",
      "disbursed in an accurate and timely manner\n",
      "c\n",
      "\n",
      "SCORE: -2.8600445\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 12\n",
      "\n",
      "Of these 47 grant projects, AGO noted that there were seven projects where \n",
      "there was no evidence that EDB had followed up with the grant recipients to determine \n",
      "that the project conditions and milestones had been met by the stipulated due dates\n",
      "\n",
      "SCORE: -2.9031847\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 13\n",
      "\n",
      "Stage 2: Grant Evaluation and Approval\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant applications were properly evaluated and approved\n",
      "\n",
      "SCORE: -2.9887857\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 14\n",
      "\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "\n",
      "SCORE: -3.1901813\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 15\n",
      "\n",
      "For the same grant scheme, AGO also found that certain eligibility criteria \n",
      "were either stated inaccurately or not stated in the grant agreements with 2 companies\n",
      "\n",
      "SCORE: -3.3568773\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 16\n",
      "\n",
      "Stage 1 – Grant Design and Setup\n",
      "AGO observed that the grant eligibility criteria and operational requirements for \n",
      "the administration of the grant schemes were properly laid down in legislation or \n",
      "implementation documents\n",
      "\n",
      "SCORE: -3.3974187\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 17\n",
      "\n",
      "Audit Observations\n",
      "Main Findings\n",
      "A substantial portion of the audit findings pertains to procurement and contract \n",
      "management, and financial administration\n",
      "\n",
      "SCORE: -3.4839458\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 18\n",
      "\n",
      "There was also inadequate assessment of the proposed costs to be supported \n",
      "and verification of grant applicants’ eligibility\n",
      "\n",
      "SCORE: -3.5611262\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 19\n",
      "\n",
      "Stage 1: Grant Design and Setup\n",
      "– whether processes were in place to ensure that grant programmes \n",
      "were authorised and reviewed for relevance\n",
      "b\n",
      "\n",
      "SCORE: -3.7886286\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "RANK: 20\n",
      "\n",
      "AGO found inconsistent practices across PPs in their stipulation of \n",
      "requirements to grant recipients and their checks performed on grant applications\n",
      "\n",
      "SCORE: -3.8983538\n",
      "\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reranking ------------------------------------------------------------------\n",
    "\n",
    "# top n matches for reranking\n",
    "top_n = 20\n",
    "\n",
    "# Cross encoder model\n",
    "\n",
    "# claimed to be deprecated because it is bad but seems to still work fine\n",
    "# model_name = \"cross-encoder/stsb-roberta-base\"\n",
    "\n",
    "# best performing on Microsoft tests\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "\n",
    "# Can be run multiple times when change in query\n",
    "# Reranking\n",
    "best_chunks, scores = reranking(model_name, good_chunks, query, top_n)\n",
    "\n",
    "print(\"RERANKING-----------------------------------------------------------\\n\")\n",
    "pretty_print_rank(best_chunks, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Augment\n",
    "\n",
    "[\"../utils/prompt_engineering.py\"](../utils/prompt_engineering.py) performs prompt engineering to augment all the retrieved chunks together. There are 2 prompt engineering examples provided, for AGO reports and National Day Rally speeches. \n",
    "\n",
    "This notebook also provides an example of a bad prompt (without prompt engineering and adding metadata.) [\"../notebooks/RAG_db.ipynb\"](../notebooks/RAG_db.ipynb) compares the output of good and bad prompts (at the end). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.prompt_engineering import generate_prompt\n",
    "from utils.json_parser import json_file_to_dict\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import save_inverted_tree_path, s_p_pairs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chunks: 8210\n",
      "s_p_pairs will be filled\n"
     ]
    }
   ],
   "source": [
    "# Chunk into sentences ('s') or paragraphs ('p') or fixed-size strings ('f')\n",
    "chunking='s' \n",
    "\n",
    "# Group smaller chunks into a bigger chunk\n",
    "grouping=1\n",
    "\n",
    "# question by user\n",
    "# query can be set to be same as user's question or \n",
    "#  by using HyDE, a hypothetical answer to the user's question\n",
    "question = query\n",
    "\n",
    "# RUN ONCE\n",
    "# retrieve all required data structures\n",
    "\n",
    "# load tree\n",
    "inverted_tree = json_file_to_dict(save_inverted_tree_path)\n",
    "\n",
    "# load chunks from tree's keys\n",
    "chunks = list(inverted_tree.keys())\n",
    "print(\"Number of unique chunks:\", len(chunks))\n",
    "\n",
    "# load sentence paragraph pairs. \n",
    "if (chunking == 's' or chunking == 'f') and grouping == 1:\n",
    "    print(\"s_p_pairs will be filled\")\n",
    "    s_p_pairs = json_file_to_dict(s_p_pairs_path)\n",
    "else:\n",
    "    s_p_pairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role:\n",
      "You are a specialist who uses the context provided to answer the query.\n",
      "\n",
      "Instruction:\n",
      "Your response should cite sources' year and page number.\n",
      "If possible, make ministries or government agencies the headings.\n",
      "If you are unable to provide an answer, state \"Unable to find, submit prompt again.\"\n",
      "\n",
      "Background:\n",
      "The context is taken from audit reports from the Auditor-General's Office (AGO) of Singapore. \n",
      "AGO is an independent organ of state and the national auditor. They play an important role in enhancing public accountability in the management and use of public funds and resources through their audits.\n",
      "\n",
      "They audit\n",
      "    government ministries and departments\n",
      "    organs of state\n",
      "    statutory boards\n",
      "    government funds\n",
      "    other public authorities and bodies administering public funds (upon their request for audit), e.g. government-owned companies.\n",
      "\n",
      "They report their audit observations to the President, Parliament and the public through the Annual Report of the Auditor-General management of the organisations audited through management letters.\n",
      "Their observations include system weaknesses, non-compliance with control procedures or legislation, and instances of excess, extravagance, or gross inefficiency leading to waste in the use of public funds and resources.\n",
      "\n",
      "CONTEXT\n",
      "Context 0:\n",
      "Year: 2016_17\n",
      "Location in document: PART II : AUDIT OF STATUTORY BOARDS, MINISTRY OF TRADE AND INDUSTRY, Economic Development Board, Lapses in Administration of Grants\n",
      "Page number: 48\n",
      "Content: Pertaining to the lack of checks on declarations by grant recipients, EDB \n",
      "explained that there were specific controls in place to ensure that grant recipients take \n",
      "ownership for accurate and credible reporting.  These included sample checks with \n",
      "onsite visits by its Internal Audit, and conduct of site visits by its Cluster Groups for \n",
      "those incentive schemes involving support for equipment or materials.  AGO noted \n",
      "that the site visits by EDB’s Cluster Groups would apply to five of the nine schemes \n",
      "audited by AGO\n",
      "\n",
      "Context 1:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 54\n",
      "Content: The audit examined whether there was a proper framework for grant \n",
      "management and whether due process was followed for the above stages.  The audit \n",
      "did not seek to certify whether the grant recipients had, in all material aspects, used or \n",
      "managed the grants in accordance with the grant terms and conditions.  For grants \n",
      "which were managed by WSG and ESG jointly with their programme partners (PPs) \n",
      "such as Trade Associations and Chambers (TACs), the audit focus was on WSG’s \n",
      "and ESG’s roles and responsibilities in the grant management\n",
      "\n",
      "Context 2:\n",
      "Year: 2017_18\n",
      "Location in document: PART III: THEMATIC AUDIT, SUMMARY\n",
      "Page number: 45\n",
      "Content: The audit examined whether there was a proper framework for grant \n",
      "management and whether due process was followed for the above stages by the two \n",
      "agencies.  For grants that were jointly managed by A*STAR/NRF and one or more \n",
      "other agencies, the audit focus was on A*STAR/NRF’s role and responsibilities in \n",
      "the grant management\n",
      "\n",
      "Context 3:\n",
      "Year: 2022_23\n",
      "Location in document: PART III : THEMATIC AUDIT – COVID-19 RELATED GRANTS, SUMMARY\n",
      "Page number: 49\n",
      "Content: Stage 4: Grant Monitoring and Review\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grants were managed in accordance with relevant terms \n",
      "and conditions, and that the deliverables were achieved\n",
      "\n",
      "Context 4:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 54\n",
      "Content: Stage 4: Grant Monitoring and Review\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grants were managed in accordance with relevant terms and conditions, \n",
      "and that the deliverables were achieved\n",
      "\n",
      "Context 5:\n",
      "Year: 2018_19\n",
      "Location in document: OVERVIEW, SUMMARY\n",
      "Page number: 7\n",
      "Content: (3)\t\n",
      "Gaps in Management of Social Grant Programmes\n",
      "AGO carried out a thematic audit on selected social grant programmes managed by \n",
      "MOH and MSF.  A total of $1.59 billion was disbursed by the two ministries under \n",
      "their social grant programmes to 1,058 Programme-Voluntary Welfare Organisations2 \n",
      "(VWOs) during the two-year period 1 April 2016 to 31 March 2018.  Of these, AGO \n",
      "test-checked 429 Programme-VWOs covering disbursement value of $488.52 million \n",
      "(or 30.7 per cent).  The audit covered five stages of grant management, namely, \n",
      "(i) grant design and setup, (ii) grant evaluation and approval, (iii) disbursement of grants, \n",
      "(iv) monitoring and review of grants, and (v) cessation of grants\n",
      "\n",
      "Context 6:\n",
      "Year: 2014_15\n",
      "Location in document: OVERVIEW, SUMMARY\n",
      "Page number: 3\n",
      "Content: Audit Observations\n",
      "In this year’s audits, AGO uncovered a number of instances that indicated laxity in \n",
      "the administration of grants.  The common weaknesses observed include failure by \n",
      "the public sector entities to ensure that the correct amount of grants are disbursed \n",
      "and conditions for grants are adhered to.  For proper accountability, it is important \n",
      "that controls and proper mechanisms are in place to ensure that grants are used for \n",
      "the intended purposes\n",
      "\n",
      "Context 7:\n",
      "Year: 2021_22\n",
      "Location in document: PART II : AUDIT OF STATUTORY BOARDS, MINISTRY OF SUSTAINABILITY AND THE ENVIRONMENT, National Environment Agency, Possible Irregularities in Quotations Submitted for Grant Applications\n",
      "Page number: 46\n",
      "Content: The grant scheme aims to raise operational efficiency and productivity of the \n",
      "environmental services industry through technology adoption.  The scheme provides \n",
      "co-funding support up to a certain percentage of qualifying costs, with a total grant \n",
      "cap per company.  To apply for the grant, applicants are required to identify the \n",
      "equipment or digital solution that they intended to procure.  Among other things, \n",
      "applicants are required to submit a few quotations for the identified equipment or \n",
      "solution to demonstrate cost reasonableness\n",
      "\n",
      "Context 8:\n",
      "Year: 2017_18\n",
      "Location in document: PART III: THEMATIC AUDIT, SUMMARY\n",
      "Page number: 45\n",
      "Content: Application, evaluation and award of grants \n",
      "– whether the processes to invite, receive, evaluate and approve \n",
      "proposals and contract with grant recipients2 were properly administered\n",
      "b\n",
      "\n",
      "Context 9:\n",
      "Year: 2022_23\n",
      "Location in document: PART III : THEMATIC AUDIT – COVID-19 RELATED GRANTS, SUMMARY\n",
      "Page number: 48\n",
      "Content: Stage 2: Grant Evaluation and Approval\n",
      "–\t\n",
      "Whether there were processes and controls in place to ensure \n",
      "that grant cases were properly evaluated and approved; and\n",
      "–\t\n",
      "Whether proper terms and conditions were stipulated for \n",
      "compliance\n",
      "\n",
      "Context 10:\n",
      "Year: 2017_18\n",
      "Location in document: PART III: THEMATIC AUDIT, SUMMARY\n",
      "Page number: 45\n",
      "Content: Disbursement of grants \n",
      "– whether the processes were in place to ensure that grants were \n",
      "disbursed in an accurate and timely manner\n",
      "c\n",
      "\n",
      "Context 11:\n",
      "Year: 2016_17\n",
      "Location in document: PART II : AUDIT OF STATUTORY BOARDS, MINISTRY OF TRADE AND INDUSTRY, Economic Development Board, Lapses in Administration of Grants\n",
      "Page number: 46\n",
      "Content: Of these 47 grant projects, AGO noted that there were seven projects where \n",
      "there was no evidence that EDB had followed up with the grant recipients to determine \n",
      "that the project conditions and milestones had been met by the stipulated due dates\n",
      "\n",
      "Context 12:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 53\n",
      "Content: Stage 2: Grant Evaluation and Approval\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant applications were properly evaluated and approved; and\n",
      "– whether agreements with grant recipients were properly entered into\n",
      "\n",
      "Context 13:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 53\n",
      "Content: Stage 1: Grant Design and Setup\n",
      "– whether there were processes and controls in place to ensure that \n",
      "grant programmes were authorised and administered in accordance \n",
      "with the objective(s) of the grant\n",
      "\n",
      "Context 14:\n",
      "Year: 2022_23\n",
      "Location in document: PART II : AUDIT OF STATUTORY BOARDS, MINISTRY OF TRANSPORT, Civil Aviation Authority of Singapore, Lapses in Management of Grants\n",
      "Page number: 42\n",
      "Content: For the same grant scheme, AGO also found that certain eligibility criteria \n",
      "were either stated inaccurately or not stated in the grant agreements with 2 companies\n",
      "\n",
      "Context 15:\n",
      "Year: 2022_23\n",
      "Location in document: OVERVIEW, SUMMARY\n",
      "Page number: 8\n",
      "Content: Stage 1 – Grant Design and Setup\n",
      "AGO observed that the grant eligibility criteria and operational requirements for \n",
      "the administration of the grant schemes were properly laid down in legislation or \n",
      "implementation documents.  Proper contracts and agreements were entered into with \n",
      "external parties appointed to administer the schemes’ key processes.  Approval was also \n",
      "obtained from MOF for the funding of the schemes\n",
      "\n",
      "Context 16:\n",
      "Year: 2011_12\n",
      "Location in document: OVERVIEW, SUMMARY\n",
      "Page number: 5\n",
      "Content: Audit Observations\n",
      "Main Findings\n",
      "A substantial portion of the audit findings pertains to procurement and contract \n",
      "management, and financial administration.  The lapses and irregularities point to the need \n",
      "for the public sector agencies concerned to make improvements to the following areas\n",
      "\n",
      "Context 17:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 57\n",
      "Content: There was also inadequate assessment of the proposed costs to be supported \n",
      "and verification of grant applicants’ eligibility.  AGO also noted instances where \n",
      "companies or individuals might have circumvented the grant requirements and \n",
      "controls put in place.  In AGO’s view, even though the administration of the grant \n",
      "programmes was outsourced to the PPs, WSG remains accountable for how the funds \n",
      "are managed and should maintain adequate oversight of the PPs\n",
      "\n",
      "Context 18:\n",
      "Year: 2018_19\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 54\n",
      "Content: Stage 1: Grant Design and Setup\n",
      "– whether processes were in place to ensure that grant programmes \n",
      "were authorised and reviewed for relevance\n",
      "b\n",
      "\n",
      "Context 19:\n",
      "Year: 2019_20\n",
      "Location in document: PART III : THEMATIC AUDIT, SUMMARY\n",
      "Page number: 57\n",
      "Content: Nevertheless, AGO noted some areas for improvement.  For WSG, there \n",
      "is a need for better oversight of PPs which administer grants on its behalf.  This \n",
      "is especially important as the PPs were of varying sizes and had different systems \n",
      "of controls.  AGO found inconsistent practices across PPs in their stipulation of \n",
      "requirements to grant recipients and their checks performed on grant applications\n",
      "\n",
      "Query: What are the findings pertaining to grant?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Can be run multiple times when change in query\n",
    "prompt = generate_prompt(question, \n",
    "                         inverted_tree, \n",
    "                         best_chunks, \n",
    "                         chunking, \n",
    "                         s_p_pairs)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.5: Prompt Expansion\n",
    "\n",
    "Notice how despite doing sentence based chunking, our prompt contains paragraphs instead? That is because sentences usually do not provide sufficient information to answer the user's questions. \n",
    "\n",
    "That's why I have used sentence_paragraph_pairs to replace the retrieved sentences with paragraphs for the final prompt. This also works for fixed-size chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Generate\n",
    "\n",
    "The final step of RAG is to feed the prompt into an LLM and generate an answer. I have chosen GPT-4o due to its large context window. LLMs will keep improving so remember to pick the LLM that best suits your use case. \n",
    "\n",
    "In order to keep track of LLM inputs and outputs, I have used LangSmith. The free tier is sufficient for development environments. LangSmith will also allow you to grade your LLM outputs, hence creating metrics for evaluating your chatbot performance in the future. Follow this [guide](https://docs.smith.langchain.com) to get started with LangSmith.\n",
    "\n",
    "In the utilities file, [\"../utils/langsmith_trace.py\"](../utils/langsmith_trace.py), I have documented how one can trace their functions using LangSmith. Our final chatbot (every api call) also uses LangSmith tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.langsmith_trace import rag_pipeline, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack parameters for tracing using LangSmith \n",
    "\n",
    "# control minimum chunk size\n",
    "min_chunk_size=100\n",
    "\n",
    "# add to data base in batches\n",
    "batch_size = 1000\n",
    "\n",
    "# An optional add-on, To be explained in the experiments tutorial\n",
    "HyDE = False\n",
    "if HyDE:\n",
    "    comments = \"This is using HyDE\"\n",
    "else:\n",
    "    comments = \"None\"\n",
    "\n",
    "params = (question, chunking, grouping, min_chunk_size, batch_size, top_k, \n",
    "          weights, k, top_n, model_name, HyDE, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ministry of Trade and Industry\n",
       "\n",
       "## Economic Development Board\n",
       "### Lapses in Administration of Grants\n",
       "- **Year:** 2016-17\n",
       "- **Page Number:** 46\n",
       "- **Findings:** For 47 grant projects audited by AGO, there were seven projects where there was no evidence that EDB had followed up with the grant recipients to determine that the project conditions and milestones had been met by the stipulated due dates (AGO, 2016-17, p. 46).\n",
       "\n",
       "### Lapses in Administration of Grants\n",
       "- **Year:** 2016-17\n",
       "- **Page Number:** 48\n",
       "- **Findings:** AGO noted a lack of checks on declarations by grant recipients. EDB explained specific controls in place, such as sample checks with onsite visits by its Internal Audit and conduct of site visits by its Cluster Groups for certain incentive schemes. However, these site visits would apply to only five of the nine schemes audited by AGO (AGO, 2016-17, p. 48).\n",
       "\n",
       "# Ministry of Transport\n",
       "\n",
       "## Civil Aviation Authority of Singapore\n",
       "### Lapses in Management of Grants\n",
       "- **Year:** 2022-23\n",
       "- **Page Number:** 42\n",
       "- **Findings:** Certain eligibility criteria were either inaccurately stated or not included in the grant agreements with two companies (AGO, 2022-23, p. 42).\n",
       "\n",
       "# Ministry of Sustainability and the Environment\n",
       "\n",
       "## National Environment Agency\n",
       "### Possible Irregularities in Quotations Submitted for Grant Applications\n",
       "- **Year:** 2021-22\n",
       "- **Page Number:** 46\n",
       "- **Findings:** The grant scheme aims to raise operational efficiency and productivity in the environmental services industry through technology adoption. Among other things, applicants are required to submit a few quotations for the identified equipment or solution to demonstrate cost reasonableness (AGO, 2021-22, p. 46).\n",
       "\n",
       "# Ministry of Health and Ministry of Social and Family Development\n",
       "\n",
       "### Management of Social Grant Programmes\n",
       "- **Year:** 2018-19\n",
       "- **Page Number:** 7\n",
       "- **Findings:** AGO identified gaps in the management of social grant programmes. $1.59 billion was disbursed by MOH and MSF under their social grant programmes to 1,058 Programme-Voluntary Welfare Organisations (VWOs) from 1 April 2016 to 31 March 2018. AGO test-checked 429 Programme-VWOs covering a disbursement value of $488.52 million (30.7 per cent). The audit covered five stages of grant management: grant design and setup, grant evaluation and approval, disbursement of grants, monitoring and review of grants, and cessation of grants (AGO, 2018-19, p. 7).\n",
       "\n",
       "# Workforce Singapore (WSG) and Enterprise Singapore (ESG)\n",
       "\n",
       "### Roles and Responsibilities in Grant Management\n",
       "- **Year:** 2019-20\n",
       "- **Page Number:** 54\n",
       "- **Findings:** The audit examined whether there was a proper framework for grant management and whether due process was followed. For grants managed jointly by WSG and ESG with their programme partners (PPs), such as Trade Associations and Chambers, the audit focus was on the roles and responsibilities of WSG and ESG in grant management (AGO, 2019-20, p. 54).\n",
       "\n",
       "### Administration of Grant Programmes by WSG\n",
       "- **Year:** 2019-20\n",
       "- **Page Number:** 57\n",
       "- **Findings:** There was an inadequate assessment of proposed costs to be supported and verification of grant applicants' eligibility. AGO also noted instances where companies or individuals might have circumvented the grant requirements and controls. Despite the administration of grant programmes being outsourced to PPs, WSG remains accountable for how the funds are managed and should maintain adequate oversight of the PPs (AGO, 2019-20, p. 57).\n",
       "\n",
       "### Oversight of Programme Partners\n",
       "- **Year:** 2019-20\n",
       "- **Page Number:** 57\n",
       "- **Findings:** WSG needs better oversight of PPs administering grants on its behalf. PPs of varying sizes with different control systems led to inconsistent practices in stipulating requirements and performing checks on grant applications (AGO, 2019-20, p. 57).\n",
       "\n",
       "# General Findings\n",
       "\n",
       "### Common Weaknesses in Grant Administration\n",
       "- **Year:** 2014-15\n",
       "- **Page Number:** 3\n",
       "- **Findings:** AGO uncovered several instances indicating laxity in the administration of grants. Common weaknesses include the failure to ensure that the correct amount of grants is disbursed and that conditions for grants are adhered to (AGO, 2014-15, p. 3).\n",
       "\n",
       "### Procurement and Contract Management\n",
       "- **Year:** 2011-12\n",
       "- **Page Number:** 5\n",
       "- **Findings:** A substantial portion of audit findings pertained to procurement and contract management, and financial administration. Lapses and irregularities point to the need for improvements in these areas (AGO, 2011-12, p. 5).\n",
       "\n",
       "# Anonymous Observations\n",
       "\n",
       "### Grant Evaluation and Approval\n",
       "- **Year:** 2019-20\n",
       "- **Page Number:** 53\n",
       "- **Findings:** Audit examined whether controls ensured that grant applications were properly evaluated and approved and whether agreements with grant recipients were properly entered into (AGO, 2019-20, p. 53).\n",
       "\n",
       "### Grant Design and Setup\n",
       "- **Year:** 2019-20\n",
       "- **Page Number:** 53\n",
       "- **Findings:** Audit examined whether processes and controls were in place to ensure that grant programmes were authorised and administered in accordance with objectives (AGO, 2019-20, p. 53).\n",
       "\n",
       "### Grant Design and Setup\n",
       "- **Year:** 2022-23\n",
       "- **Page Number:** 8\n",
       "- **Findings:** The grant eligibility criteria and operational requirements were properly laid down in legislation or implementation documents. Proper contracts and agreements were entered into with parties administering the schemes' key processes. Approval was also obtained from the Ministry of Finance for the funding of the schemes (AGO, 2022-23, p. 8).\n",
       "\n",
       "### Grant Monitoring and Review\n",
       "- **Year:** 2022-23\n",
       "- **Page Number:** 49\n",
       "- **Findings:** Processes and controls were examined to ensure that grants were managed in accordance with relevant terms and conditions and that deliverables were achieved (AGO, 2022-23, p. 49).\n",
       "\n",
       "Unable to find, submit prompt again."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I get a deadlock error when LLM is initialised in another file. Is this ok?\n",
    "# yes, it's ok. Doesn't affect the LLM outputs. So just disable parallelism since not needed \n",
    "'''\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "'''\n",
    "\n",
    "# prevents deadlocks (corrects for the above error)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "response = rag_pipeline(params, prompt, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete RAG Pipeline\n",
    "We have provided some notebooks where you can run the whole RAG pipeline\n",
    "\n",
    "1. [\"../notebooks/RAG_db.ipynb\"](../notebooks/RAG_db.ipynb) \n",
    "    - Uses AGO audit reports\n",
    "    - Uses both chromadb and ElasticSearch\n",
    "    - Can experiment with sentence, paragraph and fized size chunking\n",
    "    - Experiment with extent of prompt engineering\n",
    "\n",
    "2. [\"../notebooks/RAG_db_NDR.ipynb\"](../notebooks/RAG_db_NDR.ipynb) \n",
    "    - Uses National Day Rally (NDR) speech 2024. \n",
    "    - Does not use content pages (not available in NDR speech)\n",
    "\n",
    "3. [\"../notebooks/RAG_langchain.ipynb\"](../notebooks/RAG_langchain.ipynb)\n",
    "    - Does RAG using the langchain environment  \n",
    "    - Langchain FAISS (dense retrieval)\n",
    "    - Langchain BM25 (sparce retrieval)\n",
    "    - Langchain Ensemble retriever\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
