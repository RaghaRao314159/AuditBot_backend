{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (Auditbot_backend) to the system path\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.dirname(f\"{os.getcwd()}/data_processing.ipynb\"),\n",
    "            '..'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG to Build a Custom ChatBot\n",
    "## 1. Data Processing\n",
    "\n",
    "> **Notice:**  \n",
    "> Before starting this tutorial series, read up on the RAG pipeline.\n",
    "\n",
    "This tutorial series assumes prerequisite understanding of RAG and therefore goes through the implementation of an advanced and customized RAG pipeline, explaining the micro-decisions made along the way.\n",
    "\n",
    "> **Data Corpus:** \n",
    "> This tutorial uses [AGO yearly audit reports](https://www.ago.gov.sg/publications/annual-reports/) as an example. However, this repo's code is applicable to most pdf documents. The code examples for other documents (such as national day rally) will be referenced later. \n",
    "\n",
    "Also, have a look at [\"../flowchart.png\"](../flowchart.png) for an overview on the whole pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Building a data corpus\n",
    "Using http get requests and the BeautifulSoup library, download of all audit reports from the AGO website can be automated. There are also infographics files on the website that I have decided to ignore as they repeat the content from the actual audit reports. Howevever, feel free to use them as well. \n",
    "\n",
    "The code for this can be found in [\"../notebooks/web_scraper.ipynb\"](../notebooks/web_scraper.ipynb). This notebook also extracts the content pages from the sudit reports as content pages provide hierarchical information that is useful (analysed later).\n",
    "\n",
    "Your data directory should now be arranged as:\n",
    "\n",
    "```md\n",
    "Main_dir/\n",
    "└── data/\n",
    "    └── documents/\n",
    "        ├── ar_fy2008_09_content_pages.pdf\n",
    "        ├── ar_fy2008_09.pdf\n",
    "        ├── ar_fy2009_10_content_pages.pdf\n",
    "        ├── ar_fy2009_10.pdf\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        └── ar_fy2022_23.pdf\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Converting pdf to text\n",
    "There are two main approaches to this, Optical Character Recognition (OCR) and pdf readers\n",
    "\n",
    "#### OCR\n",
    "OCR uses computer vision techniques to convert image to text. However, current OCR techniques are not accurate enough to achieve 100% conversion to text. It struggles greatly with new lines, paragraphs and page numbers, which are all important for chunking. Even vision transformers and GPT4-o have not achieved this. \n",
    "\n",
    "Check out [\"../notebooks/ocr.ipynb\"](../notebooks/ocr.ipynb) to try OCR for yourself. It might be satisfactory for your use case.\n",
    "\n",
    "#### PDF Readers\n",
    "The most accurate pdf reader as of now is PyMuPDF.\n",
    "```bash\n",
    "!pip install PyMuPDF \n",
    "```\n",
    "```python\n",
    "import fitz\n",
    "pdf_document_ = fitz.open(pdf_path)\n",
    "```\n",
    "Helper functions ```pdf_to_pages(pdf_path)``` and ```pdf_to_text(pdf_path)``` have been provided in [\"../utils/preprocessing.py\"](../utils/preprocessing.py). They are also later used in chunking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Hierarchial structure for Documents\n",
    "```json\n",
    "{\n",
    "    \"2022_23\": {\n",
    "        \"OVERVIEW\": {\n",
    "            \"SUMMARY\": \"pages 1 to 12\"\n",
    "        },\n",
    "        \"PART I A : AUDIT OF GOVERNMENT FINANCIAL STATEMENTS\": {\n",
    "            \"SUMMARY\": \"pages 13 to 14\"\n",
    "        },\n",
    "        \"PART I B : AUDIT OF GOVERNMENT MINISTRIES, ORGANS OF STATE AND GOVERNMENT FUNDS\": {\n",
    "            \"SUMMARY\": \"pages 15 to 16\",\n",
    "            \"MINISTRY OF COMMUNICATIONS AND INFORMATION\": {\n",
    "                \"Tenderers Appointed Despite Not Meeting Evaluation Criteria\": \"page 18\"\n",
    "                \n",
    "            }\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "        }\n",
    "    }\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "}\n",
    "```\n",
    "\n",
    "This tree will be extremely useful in determing where the chunks originated from and providing metadata to the LLM. The code to generate this tree can be found in [\"../notebooks/content_page_parser.ipynb\"](../notebooks/content_page_parser.ipynb). \n",
    "\n",
    "> **Notice:**  \n",
    "> Only documents with content pages can generate this tree as content pages provide hierarchial information. Documents without content pages will lose out on the metadata provided by this tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Chunking\n",
    "I used sentence based chunking as it provides the best results for the AGO reports. Other documents might perform better with alternative chunking methods. Explore the different chunking methods in [../notebooks/text_splitter.ipynb](../notebooks/text_splitter.ipynb). \n",
    "\n",
    "> **FYI**  \n",
    "> Although not proven, it is experiemntally sugested that chunks of similar size to questions will rank higher by dense retrievers. Therefore, it is not surprising that this project observed sentence based chunking produce the best results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for chunking\n",
    "from utils.preprocessing import generate_chunks\n",
    "\n",
    "# contains bunch of constants such as saving paths \n",
    "from utils.initialisations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7536468505859375 seconds\n",
      "number of chunks: 9127\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS ============================================================\n",
    "# preprocessing --------------------------------------------------------------\n",
    "\n",
    "# Chunk into sentences ('s') or paragraphs ('p')\n",
    "chunking='s' \n",
    "\n",
    "# Group smaller chunks into a bigger chunk\n",
    "grouping=1\n",
    "\n",
    "# control minimum chunk size\n",
    "min_chunk_size=100\n",
    "\n",
    "# RUN ONCE\n",
    "# generate chunks and other useful data structures\n",
    "generate_chunks(DOCUMENT_DIR,\n",
    "                chunks_path,\n",
    "                chunk_pageNum_pairs_path,\n",
    "                s_p_pairs_path, \n",
    "                chunking, \n",
    "                grouping, \n",
    "                min_chunk_size,\n",
    "                DOC_IDENTIFIER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Retrieving Metadata on chunks\n",
    "Having more metadata on the chunks means more information is provided to the final LLM and hence the more acurate the answer. Example:\n",
    "| Chunk   | Source             | Year | Location in Document                                                                                                                                               | Page Number |\n",
    "|---------|--------------------|------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n",
    "| chunk 1 | ar_fy2018_19.pdf    | 2019 | **Part 1B: Audit of Government Ministries, Organs of State and Government Funds** <br> *Ministry of Education* <br> Connect Fund | 24          |\n",
    "\n",
    "The source document, year and page number can be easily determined during chunking by using string methods. However, Location in document, which potentially provides key information is very difficult to obtain directly from pdf readers.\n",
    "\n",
    "I have instead made use of the tree structure obtained from the content page to fill in the \"Location in Document\" metadata. Documents that do not contain content pages will miss out on this metadata. The following code will create a dictionary (inverted tree) where the keys are the chunks and the values are the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.content_page_parser import generate_inverted_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate inverted tree\n",
    "has_content_page = True\n",
    "generate_inverted_tree(chunk_pageNum_pairs_path, \n",
    "                       has_content_page, \n",
    "                       save_inverted_tree_path,\n",
    "                       tree_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [../notebooks/preprocessing.ipynb](../notebooks/preprocessing.ipynb) to understand how metadata for chunks can be retrieved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
