{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (Auditbot_backend) to the system path\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.dirname(f\"{os.getcwd()}/database_setup.ipynb\"),\n",
    "            '..'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG to Build a Custom ChatBot\n",
    "## 2. Database Setup\n",
    "\n",
    "> **Notice:**  \n",
    "> Before starting this tutorial series, read up on the RAG pipeline.\n",
    "\n",
    "This tutorial series assumes prerequisite understanding of RAG and therefore goes through the implementation of an advanced and customized RAG pipeline, explaining the micro-decisions made along the way.\n",
    "\n",
    "> **Data Corpus:** \n",
    "> This tutorial uses [AGO yearly audit reports](https://www.ago.gov.sg/publications/annual-reports/) as an example. However, this repo's code is applicable to most pdf documents. The code examples for other documents (such as national day rally) will be referenced later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve all required data structures\n",
    "\n",
    "In the previous tutorial, we generated chunks and stored them as json files. However for production, data bases will be required. Therefore, data has to be retrireved from these json files and transferred to appropriate databases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom helper functions\n",
    "from utils.json_parser import json_file_to_dict\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import save_inverted_tree_path, s_p_pairs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chunks: 8210\n",
      "s_p_pairs will be filled\n"
     ]
    }
   ],
   "source": [
    "# Chunk into sentences ('s') or paragraphs ('p') or fixed-size strings ('f')\n",
    "chunking='s' \n",
    "\n",
    "# Group smaller chunks into a bigger chunk\n",
    "grouping=1\n",
    "\n",
    "# RUN ONCE\n",
    "# retrieve all required data structures\n",
    "\n",
    "# load tree\n",
    "inverted_tree = json_file_to_dict(save_inverted_tree_path)\n",
    "\n",
    "# load chunks from tree's keys\n",
    "chunks = list(inverted_tree.keys())\n",
    "print(\"Number of unique chunks:\", len(chunks))\n",
    "\n",
    "# load sentence paragraph pairs. \n",
    "if (chunking == 's' or chunking == 'f') and grouping == 1:\n",
    "    print(\"s_p_pairs will be filled\")\n",
    "    s_p_pairs = json_file_to_dict(s_p_pairs_path)\n",
    "else:\n",
    "    s_p_pairs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fill up vector datastore\n",
    "\n",
    "I used Chroma as the go to vector store as it is free, easy to setup and built ontop of sqlite3. It also provides various bi-encoders to generate vector embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromadb library\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# custom helper functions\n",
    "from utils.db_utils import (chroma_get_or_create_collection, \n",
    "                            chroma_fill_db,\n",
    "                            chroma_preprocess_metadata)\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embeddings in database: 8210\n"
     ]
    }
   ],
   "source": [
    "# vector store ---------------------------------------------------------------\n",
    "\n",
    "# add to data base in batches\n",
    "batch_size = 1000\n",
    "\n",
    "# prepare metadata for chromadb\n",
    "pre_metadata = list(inverted_tree.values())\n",
    "metadata = chroma_preprocess_metadata(pre_metadata)\n",
    "\n",
    "# RUN ONCE\n",
    "# set up vector database for dense embedding search\n",
    "\n",
    "# chromadb supported model\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "\n",
    "# create db\n",
    "# When creating the data base for the first time, make sure the data base is \n",
    "# reset (all collections are erased)\n",
    "client_dense = chromadb.PersistentClient(path=\"../data/db\", \n",
    "                                   settings = Settings(allow_reset=True))\n",
    "\n",
    "# chromadb's embedding function needs streaming in batches\n",
    "# Basically, chunks are added in batches\n",
    "collection = chroma_get_or_create_collection(client_dense, \n",
    "                                             name = \"audit\", \n",
    "                                             embedding_function = openai_ef, \n",
    "                                             reset = True)\n",
    "\n",
    "# fill db\n",
    "chroma_fill_db(collection, chunks, metadata, batch_size)\n",
    "print(\"number of embeddings in database:\",collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to quickly test ChromaDB, a mock up RAG pipeline has been written in [\"../notebooks/chroma_db.ipynb\"](../notebooks/chroma_db.ipynb). No other setup other than chromadb is required. \n",
    "\n",
    "This notebook also includes an alternative to the embedding functions provided by chroma. This alternative (by langchain) avoids streaming into chromadb in batches. If adding data in batches is not possible for your use case, use this iinstead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create index for sparce retrieval\n",
    "\n",
    "Elasticsearch provides a wide range of options for sparce retrieval methods as well as indexing to speed up sparce retrievals. Follow the [ElasticSearch tutorial](https://www.elastic.co/guide/en/elasticsearch/reference/current/run-elasticsearch-locally.html) to create a container. A local development set-up for the container has been used in this notebook but the python code remains the same even if a production development setup is used. \n",
    "\n",
    "Bash script to start a container:\n",
    "```bash\n",
    "export ELASTIC_PASSWORD=\"<ES_PASSWORD>\"  # password for \"elastic\" username\n",
    "\n",
    "docker run -p 127.0.0.1:9200:9200 -d --name elasticsearch --network elastic-net \n",
    "  -e ELASTIC_PASSWORD=$ELASTIC_PASSWORD \n",
    "  -e \"discovery.type=single-node\" \n",
    "  -e \"xpack.security.http.ssl.enabled=false\" \n",
    "  -e \"xpack.license.self_generated.type=trial\" \n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.14.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic search library\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# custom helper functions\n",
    "from utils.db_utils import index_elastic_db\n",
    "\n",
    "# constants\n",
    "from utils.initialisations import LOCAL_HOST_URL, HTTP_AUTH, index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '50cd72118574', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'l-puMSQDTvqL7nQDfYreOg', 'version': {'number': '8.14.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'd55f984299e0e88dee72ebd8255f7ff130859ad0', 'build_date': '2024-07-07T22:04:49.882652950Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "reset index: chromadb_documents\n"
     ]
    }
   ],
   "source": [
    "# RUN ONCE\n",
    "# connect to the Elasticsearch cluster from python elasticsearch client\n",
    "client_sparce = Elasticsearch(\n",
    "    LOCAL_HOST_URL,\n",
    "    basic_auth=HTTP_AUTH\n",
    ")\n",
    "# checks if client is connected to docker container\n",
    "print(client_sparce.info(http_auth=HTTP_AUTH))\n",
    "\n",
    "# index chunks using elasticsearch (saved in docker)\n",
    "index_elastic_db(client_sparce, index_name, HTTP_AUTH, chunks, reset = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"../notebooks/elasticsearch.ipynb\"](../notebooks/elasticsearch.ipynb) offers code to look into the index and explore its functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
